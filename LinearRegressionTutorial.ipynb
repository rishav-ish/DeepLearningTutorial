{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be making a simple linear regression model using tensor flow.\n",
    "\n",
    "<strong>Linear Regression</strong> models are used to show or predict the relationship between two variables or factors.\n",
    "The Simple regression model is represented by:\n",
    "\n",
    "<i><b>y = bias + weight * x</b></i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data\n",
    "\n",
    "Before implementing the simple linear regression model let's first create out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.random.randn(2000,10)   #2000 rows and 10 columns\n",
    "w = np.random.randn(1,10)      #assigning weight vector\n",
    "b = -0.8                         #any random bias\n",
    "\n",
    "#calculating our y_data or true y\n",
    "\n",
    "y_data = np.matmul(x_data,w.T) + biased     #it will gives values in a shape 2000 X 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Model\n",
    "\n",
    "Now here we will be creating our model using tensor flow, we will be using placeholder, variable, session and Gradient Descent optimizer for creating our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- 0 --------------------------------------\n",
      "weight [[ 0.02981151  0.43139172  0.53622156 -0.36803475 -0.9189421   0.1092513\n",
      "  -0.37622112  0.06575792 -0.3299995  -0.16880251]]\n",
      "bias -0.33350325\n",
      "------------------------- 1 --------------------------------------\n",
      "weight [[ 0.04132396  0.67929226  0.8388109  -0.5867117  -1.4885838   0.15546077\n",
      "  -0.6037308   0.10365273 -0.5271901  -0.25681317]]\n",
      "bias -0.528571\n",
      "------------------------- 2 --------------------------------------\n",
      "weight [[ 0.04393287  0.8216894   1.0090469  -0.7174615  -1.8421837   0.1722578\n",
      "  -0.74117893  0.12538373 -0.6453503  -0.3022191 ]]\n",
      "bias -0.64248335\n",
      "------------------------- 3 --------------------------------------\n",
      "weight [[ 0.04262934  0.9034336   1.1044489  -0.7961169  -2.0619924   0.1761771\n",
      "  -0.8241468   0.13779421 -0.7163258  -0.32534   ]]\n",
      "bias -0.7088765\n",
      "------------------------- 4 --------------------------------------\n",
      "weight [[ 0.03994764  0.9503193   1.1576467  -0.84371233 -2.1988344   0.17506963\n",
      "  -0.8741904   0.14485832 -0.7590476  -0.3369195 ]]\n",
      "bias -0.74748474\n",
      "------------------------- 5 --------------------------------------\n",
      "weight [[ 0.03708855  0.97718066  1.1871208  -0.87267464 -2.284155    0.17247091\n",
      "  -0.9043536   0.14886947 -0.7848082  -0.3425946 ]]\n",
      "bias -0.7698742\n",
      "------------------------- 6 --------------------------------------\n",
      "weight [[ 0.03455438  0.99254715  1.2033162  -0.890392   -2.337434    0.16983613\n",
      "  -0.9225217   0.15114383 -0.80036426 -0.34529555]]\n",
      "bias -0.7828152\n",
      "------------------------- 7 --------------------------------------\n",
      "weight [[ 0.03249971  1.0013213   1.2121195  -0.90128434 -2.3707569   0.1676615\n",
      "  -0.9334574   0.15243308 -0.80976945 -0.3465281 ]]\n",
      "bias -0.7902652\n",
      "------------------------- 8 --------------------------------------\n",
      "weight [[ 0.0309194   1.0063194   1.2168362  -0.90801173 -2.3916311   0.16603199\n",
      "  -0.9400351   0.15316473 -0.81546134 -0.34705505]]\n",
      "bias -0.7945332\n",
      "------------------------- 9 --------------------------------------\n",
      "weight [[ 0.02974567  1.009158    1.2193143  -0.9121845  -2.4047282   0.16487883\n",
      "  -0.9439886   0.15358107 -0.8189086  -0.3472556 ]]\n",
      "bias -0.7969637\n",
      "------------------------- 10 --------------------------------------\n",
      "weight [[ 0.0288953   1.0107641   1.2205806  -0.9147829  -2.4129586   0.16409372\n",
      "  -0.946363    0.15381908 -0.8209975  -0.3473136 ]]\n",
      "bias -0.7983376\n",
      "------------------------- 11 --------------------------------------\n",
      "weight [[ 0.02829053  1.0116687   1.2212012  -0.91640675 -2.418139    0.16357423\n",
      "  -0.9477877   0.15395606 -0.8222638  -0.34731534]]\n",
      "bias -0.799107\n",
      "------------------------- 12 --------------------------------------\n",
      "weight [[ 0.02786656  1.0121752   1.2214853  -0.9174249  -2.4214046   0.16323823\n",
      "  -0.9486418   0.15403558 -0.8230316  -0.34729913]]\n",
      "bias -0.79953283\n",
      "------------------------- 13 --------------------------------------\n",
      "weight [[ 0.0275727   1.0124567   1.2215995  -0.9180652  -2.4234664   0.163025\n",
      "  -0.9491532   0.15408228 -0.82349724 -0.34728065]]\n",
      "bias -0.7997648\n",
      "------------------------- 14 --------------------------------------\n",
      "weight [[ 0.02737091  1.0126116   1.2216319  -0.91846895 -2.4247704   0.16289203\n",
      "  -0.949459    0.15411006 -0.8237796  -0.34726527]]\n",
      "bias -0.7998887\n",
      "------------------------- 15 --------------------------------------\n",
      "weight [[ 0.02723338  1.0126959   1.2216281  -0.9187242  -2.425596    0.16281042\n",
      "  -0.94964164  0.15412684 -0.82395077 -0.3472541 ]]\n",
      "bias -0.79995286\n",
      "------------------------- 16 --------------------------------------\n",
      "weight [[ 0.02714026  1.012741    1.2216108  -0.9188858  -2.4261198   0.16276112\n",
      "  -0.94975054  0.15413713 -0.8240545  -0.34724665]]\n",
      "bias -0.7999847\n",
      "------------------------- 17 --------------------------------------\n",
      "weight [[ 0.02707753  1.0127646   1.2215908  -0.91898847 -2.4264524   0.16273187\n",
      "  -0.94981533  0.15414356 -0.82411736 -0.34724194]]\n",
      "bias -0.7999994\n",
      "------------------------- 18 --------------------------------------\n",
      "weight [[ 0.02703548  1.0127765   1.2215724  -0.91905373 -2.426664    0.16271482\n",
      "  -0.9498538   0.15414762 -0.82415545 -0.34723914]]\n",
      "bias -0.8000053\n",
      "------------------------- 19 --------------------------------------\n",
      "weight [[ 0.02700739  1.0127822   1.2215573  -0.9190953  -2.4267988   0.16270512\n",
      "  -0.94987655  0.15415025 -0.82417846 -0.3472376 ]]\n",
      "bias -0.8000069\n"
     ]
    }
   ],
   "source": [
    "epoch = 20              #epoch is the number of iteration for which you want to train our model and optimize it.\n",
    "\n",
    "g = tf.Graph()          #creating a new tensor flow graph.\n",
    "\n",
    "with g.as_default():    #making g as a default graph\n",
    "    x = tf.placeholder(tf.float32,shape = [None,10])  #creating a placeholder for our x_data\n",
    "    y_true  = tf.placeholder(tf.float32, shape = None)# creating a placeholder for our y_data\n",
    "    \n",
    "    '''\n",
    "        INFERENCE\n",
    "        \n",
    "        Here we will assign our weight and bias randomly, right now we will be using zero.\n",
    "        Will calculate using y using the weight, bias and x.\n",
    "    \n",
    "    \n",
    "        For assigning weight and bias will be using tensor flow variable since it needed to get updated at every steps.\n",
    "        when we using loss function and doing back propogation to optimize our model.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('inference') as scope:\n",
    "        weight = tf.Variable(np.zeros((1,10)),dtype = tf.float32, name = 'weight') \n",
    "        bias   = tf.Variable(0, dtype = tf.float32, name = 'bias')\n",
    "        \n",
    "        y_pred = tf.matmul(x,tf.transpose(weight)) + bias  #calculating y for given weight and bias and x.\n",
    "        \n",
    "    '''\n",
    "        LOSS FUNCTION\n",
    "        \n",
    "        This is use to find the difference between predicted value and exact values.\n",
    "        \n",
    "        using loss function we optimize our model by minimizing the value of loss function i.e difference between\n",
    "        predicted value and exact values.\n",
    "        \n",
    "        There are various loss function availabe here we will be using MSE (Mean Squared Error).\n",
    "        \n",
    "        MSE\n",
    "        In MSE we take the square of differences between exact and predicted values and take mean of it.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_true)) #MSE, implemented using tf functions.\n",
    "        \n",
    "    '''\n",
    "        TRAINING AND OPTIMIZATION\n",
    "        \n",
    "        Here we will train our model and optimize it using Gradient Descent Optimization\n",
    "        \n",
    "        GRADIENT DESCENT OPTIMIZATION\n",
    "        \n",
    "        It is a way of differentiating cost function, and taking minimum value out of it. you can learn about it more.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    with tf.name_scope('train') as scope:\n",
    "        learning_rate = 0.2 #assigning learning rate, it is the rate at which you want your model to learn. READ ABOUT GDO\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train = optimizer.minimize(loss)\n",
    "        \n",
    "    #Executing out model\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer() #initializing our variables\n",
    "        sess.run(init)\n",
    "        for step in range(epoch):\n",
    "            sess.run(train,{x:x_data, y_true : y_data})\n",
    "            w, b = sess.run([weight,bias]) #tuple unpacking\n",
    "            print('-------------------------',step,'--------------------------------------')\n",
    "            print('weight',w)\n",
    "            print('bias',b)\n",
    "        \n",
    "        '''\n",
    "            Here we will be saving our session graph so that we can see it using tensorboard\n",
    "            we do it using tf.summary.FileWriter(path,graph)\n",
    "            \n",
    "            Go to cmd to the folder where your graph will be save and enter this command\n",
    "            \n",
    "            tensorboard --logdir foldername\n",
    "            \n",
    "            The link will be popout, paste that url to browser to see the skeletion of our graph.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        path = os.getcwd() #getting our current working directory.\n",
    "        \n",
    "        path += '\\\\Graph'\n",
    "        \n",
    "        File_writer = tf.summary.FileWriter(path,sess.graph) #it will create logg directory for your session graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our original bias and variables are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02700739  1.0127822   1.2215573  -0.9190953  -2.4267988   0.16270512\n",
      "  -0.94987655  0.15415025 -0.82417846 -0.3472376 ]] -0.8000069\n"
     ]
    }
   ],
   "source": [
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "\n",
    "you will able to see that our model at every iterations are getting better and better, in this case we get 100% accurate result because we have not embedded noise in our linear regression equations, most of the data you will find will have a noise so you will not going to achieve 100% accuracy cause that will not be possible, if it happens it means your model is overfitted.\n",
    "\n",
    "Next we will learn about logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
